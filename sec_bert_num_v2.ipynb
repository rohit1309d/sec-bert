{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohit1309d/sec-bert/blob/main/sec_bert_num_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-sgUbzBXPZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2625127e-8023-426a-d6a8-ee132446ba5a",
        "cellView": "code"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: seqeval[gpu] in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval[gpu]) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers seqeval[gpu] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEnlUbgm8z3B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm1krxJtKxpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d50db6-d6f7-4523-ab17-1ca48a938006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901A_dWLYrym",
        "outputId": "23c10434-3455-44b5-dad1-001ccb26cca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset finer139 (/root/.cache/huggingface/datasets/nlpaueb___finer139/finer-139/1.0.0/5f5a8eb2a38e8b142bb8ca63f3f9600634cc6c8963e4c982926cf2b48e4e55ff)\n",
            "Reusing dataset finer139 (/root/.cache/huggingface/datasets/nlpaueb___finer139/finer-139/1.0.0/5f5a8eb2a38e8b142bb8ca63f3f9600634cc6c8963e4c982926cf2b48e4e55ff)\n",
            "Reusing dataset finer139 (/root/.cache/huggingface/datasets/nlpaueb___finer139/finer-139/1.0.0/5f5a8eb2a38e8b142bb8ca63f3f9600634cc6c8963e4c982926cf2b48e4e55ff)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'tokens', 'ner_tags'],\n",
            "    num_rows: 900384\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "finer_train = datasets.load_dataset(\"nlpaueb/finer-139\", split=\"train\")\n",
        "finer_val = datasets.load_dataset(\"nlpaueb/finer-139\", split=\"validation\")\n",
        "finer_test = datasets.load_dataset(\"nlpaueb/finer-139\", split=\"test\")\n",
        "print(finer_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finer_val['tokens'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCKrFUWu_Ql",
        "outputId": "3c606bbf-03ad-4ec7-95e1-f5cff8ac810b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Revenue',\n",
              " 'by',\n",
              " 'segment',\n",
              " 'and',\n",
              " 'geography',\n",
              " 'for',\n",
              " 'the',\n",
              " 'three',\n",
              " 'months',\n",
              " 'ended',\n",
              " 'March',\n",
              " '31',\n",
              " ',',\n",
              " '2020',\n",
              " 'and',\n",
              " '2019',\n",
              " 'is',\n",
              " 'as',\n",
              " 'follows',\n",
              " ':',\n",
              " '8',\n",
              " 'Table',\n",
              " 'of',\n",
              " 'Contents',\n",
              " 'We',\n",
              " 'perform',\n",
              " 'our',\n",
              " 'obligations',\n",
              " 'under',\n",
              " 'a',\n",
              " 'contract',\n",
              " 'with',\n",
              " 'a',\n",
              " 'customer',\n",
              " 'by',\n",
              " 'transferring',\n",
              " 'goods',\n",
              " 'and/or',\n",
              " 'services',\n",
              " 'in',\n",
              " 'exchange',\n",
              " 'for',\n",
              " 'consideration',\n",
              " 'from',\n",
              " 'the',\n",
              " 'customer',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained('nlpaueb/sec-bert-num')\n",
        "# spacy_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# def sec_bert_shape_preprocess(text):\n",
        "#     # tokens = [t.text for t in spacy_tokenizer(sentence)]\n",
        "\n",
        "#     processed_text = []\n",
        "#     for token in tokens:\n",
        "#         if re.fullmatch(r\"(\\d+[\\d,.]*)|([,.]\\d+)\", token):\n",
        "#             shape = '[' + re.sub(r'\\d', 'X', token) + ']'\n",
        "#             if shape in tokenizer.additional_special_tokens:\n",
        "#                 processed_text.append(shape)\n",
        "#             else:\n",
        "#                 processed_text.append('[NUM]')\n",
        "#         else:\n",
        "#             processed_text.append(token)\n",
        "            \n",
        "#     return ' '.join(processed_text)\n",
        "\n",
        "# tokenized_sentence = tokenizer.tokenize(sec_bert_shape_preprocess(sentence))"
      ],
      "metadata": {
        "id": "EdmHzw6izoKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkajipeJYpWE"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "num_token = \"[NUM]\"\n",
        "\n",
        "def get_dataframe(finer):\n",
        "\n",
        "  tokens = finer['tokens']\n",
        "  for i in range(len(tokens)):   \n",
        "      for j in range(len(tokens[i])):\n",
        "          if re.fullmatch(r\"(\\d+[\\d,.]*)|([,.]\\d+)\", tokens[i][j]):\n",
        "              tokens[i][j] = num_token\n",
        "  labels = finer['ner_tags']\n",
        "\n",
        "  return pd.DataFrame(list(zip(tokens, labels)), columns =['sentence', 'word_labels'])\n",
        "\n",
        "train_dataset = get_dataframe(finer_train)\n",
        "val_dataset = get_dataframe(finer_val)\n",
        "test_dataset = get_dataframe(finer_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iob_to_labels(label):\n",
        "  return label.split('-')[-1]\n",
        "\n",
        "iob_feature_names = finer_train.features[\"ner_tags\"].feature.names\n",
        "feature_names = list(map(iob_to_labels, iob_feature_names))"
      ],
      "metadata": {
        "id": "xYrpWI9AWKYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66hmweDf-R3S"
      },
      "outputs": [],
      "source": [
        "def labelid_to_label(labelid):\n",
        "  return feature_names[labelid]\n",
        "\n",
        "def save_data_distributions(df, split):\n",
        "  df['word_labels'].apply(np.count_nonzero).value_counts().to_csv('Number of labels vs number of sentences - ' + split + '.csv', index_label = 'Number of Labels', header=['Number of sentences'])\n",
        "  pd.Series(np.concatenate(df['word_labels'].values).flat).apply(labelid_to_label).value_counts().to_csv('Labels vs Counts - ' + split + '.csv', index_label = 'Labels', header=['Counts'])\n",
        "   \n",
        "\n",
        "save_data_distributions(train_dataset, 'train')\n",
        "save_data_distributions(val_dataset, 'validation')\n",
        "save_data_distributions(test_dataset, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgNSM8Xz79Mg",
        "outputId": "8119214b-50b1-4539-ba65-478e84e665c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizerFast.from_pretrained('nlpaueb/sec-bert-num')\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': [num_token]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh3ckSO0YMZW"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.sentence[index]\n",
        "        labels = self.data.word_labels[index]\n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             is_split_into_words=True, \n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True)\n",
        "        \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "          if mapping[0] == 0 and mapping[1] != 0:\n",
        "            # overwrite label\n",
        "            encoded_labels[idx] = labels[i]\n",
        "            i += 1\n",
        "\n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrkdZBLYHVcB"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "validating_set = dataset(val_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phmPylgAm8Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbc3300-2918-4511-9b33-db8a50da3164"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'input_ids': tensor([ 102,  612, 4667,  951,  877,  621,  919,  647,  612, 1589,  626, 2899,\n",
              "         2000,  960,  816,  105,  117,  105,  703,  709, 4538, 5081,  621,  612,\n",
              "         1073,  633,  682, 1488,  647,  612, 2154,  781,  119,  103,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100]),\n",
              " 'offset_mapping': tensor([[0, 0],\n",
              "         [0, 3],\n",
              "         [0, 9],\n",
              "         ...,\n",
              "         [0, 0],\n",
              "         [0, 0],\n",
              "         [0, 0]]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "training_set[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWgnNJrYW2GP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b5e31d-ced8-4ad0-f00b-beff988bcd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       -100\n",
            "item        0\n",
            "[NUM]       0\n",
            "financial   0\n",
            "statements  0\n",
            "lennar      0\n",
            "corporation  0\n",
            "and         0\n",
            "subsidiaries  0\n",
            "condensed   0\n",
            "consolidated  0\n",
            "balance     0\n",
            "sheets      0\n",
            "(           0\n",
            "dollars     0\n",
            "in          0\n",
            "thousands   0\n",
            ",           0\n",
            "except      0\n",
            "shares      0\n",
            "and         0\n",
            "per         0\n",
            "share       0\n",
            "amounts     0\n",
            ")           0\n",
            "(           0\n",
            "unaudited   0\n",
            ")           0\n",
            "(           0\n",
            "[NUM]       0\n",
            ")           0\n",
            "under       0\n",
            "certain     0\n",
            "provisions  0\n",
            "of          0\n",
            "accounting  0\n",
            "standards   0\n",
            "codification  0\n",
            "(           0\n",
            "“           0\n",
            "asc         0\n",
            "”           0\n",
            ")           0\n",
            "topic       0\n",
            "[NUM]       0\n",
            ",           0\n",
            "consolidations  0\n",
            ",           0\n",
            "(           0\n",
            "“           0\n",
            "asc         0\n",
            "[NUM]       0\n",
            "”           0\n",
            ")           0\n",
            "the         0\n",
            "company     0\n",
            "is          0\n",
            "required    0\n",
            "to          0\n",
            "separately  0\n",
            "disclose    0\n",
            "on          0\n",
            "its         0\n",
            "condensed   0\n",
            "consolidated  0\n",
            "balance     0\n",
            "sheets      0\n",
            "the         0\n",
            "assets      0\n",
            "owned       0\n",
            "by          0\n",
            "consolidated  0\n",
            "variable    0\n",
            "interest    0\n",
            "entities    0\n",
            "(           0\n",
            "“           0\n",
            "vies        0\n",
            "”           0\n",
            ")           0\n",
            "and         0\n",
            "liabilities  0\n",
            "of          0\n",
            "consolidated  0\n",
            "vies        0\n",
            "as          0\n",
            "to          0\n",
            "which       0\n",
            "neither     0\n",
            "lennar      0\n",
            "corporation  0\n",
            ",           0\n",
            "or          0\n",
            "any         0\n",
            "of          0\n",
            "its         0\n",
            "subsidiaries  0\n",
            ",           0\n",
            "has         0\n",
            "any         0\n",
            "obligations  0\n",
            ".           0\n",
            "as          0\n",
            "of          0\n",
            "may         0\n",
            "[NUM]       0\n",
            ",           0\n",
            "[NUM]       0\n",
            ",           0\n",
            "total       0\n",
            "assets      0\n",
            "include     0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "related     0\n",
            "to          0\n",
            "consolidated  0\n",
            "vies        0\n",
            "of          0\n",
            "which       0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "is          0\n",
            "included    0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "cash        0\n",
            "and         0\n",
            "cash        0\n",
            "equivalents  0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "receivables  0\n",
            ",           0\n",
            "net         0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "finished    0\n",
            "homes       0\n",
            "and         0\n",
            "construction  0\n",
            "in          0\n",
            "progress    0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "land        0\n",
            "and         0\n",
            "land        0\n",
            "under       0\n",
            "development  0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "consolidated  0\n",
            "inventory   0\n",
            "not         0\n",
            "owned       0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       66\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "investments  0\n",
            "in          0\n",
            "unconsolidated  0\n",
            "entities    0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "other       0\n",
            "assets      0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "rialto      0\n",
            "assets      0\n",
            "and         0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "multifamily  0\n",
            "assets      0\n",
            ".           0\n",
            "as          0\n",
            "of          0\n",
            "november    0\n",
            "[NUM]       0\n",
            ",           0\n",
            "[NUM]       0\n",
            ",           0\n",
            "total       0\n",
            "assets      0\n",
            "include     0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "related     0\n",
            "to          0\n",
            "consolidated  0\n",
            "vies        0\n",
            "of          0\n",
            "which       0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "is          0\n",
            "included    0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "cash        0\n",
            "and         0\n",
            "cash        0\n",
            "equivalents  0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "receivables  0\n",
            ",           0\n",
            "net         0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "finished    0\n",
            "homes       0\n",
            "and         0\n",
            "construction  0\n",
            "in          0\n",
            "progress    0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "land        0\n",
            "and         0\n",
            "land        0\n",
            "under       0\n",
            "development  0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "consolidated  0\n",
            "inventory   0\n",
            "not         0\n",
            "owned       0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       66\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "investments  0\n",
            "in          0\n",
            "unconsolidated  0\n",
            "entities    0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "homebuilding  0\n",
            "other       0\n",
            "assets      0\n",
            ",           0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "rialto      0\n",
            "assets      0\n",
            "and         0\n",
            "$           0\n",
            "[NUM]       0\n",
            "million     0\n",
            "in          0\n",
            "lennar      0\n",
            "multifamily  0\n",
            "assets      0\n",
            ".           0\n",
            "[SEP]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n",
            "[PAD]       -100\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
        "  print('{0:10}  {1}'.format(token, label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIw793myWOmi"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validating_loader = DataLoader(validating_set, **val_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73OzU7oXRxR8"
      },
      "source": [
        "#### **Defining the model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mp6l4Khvfls",
        "outputId": "1260ffc9-b2a5-4892-fa5d-30e32e516a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " FinBERT-FinVocab-Uncased\n",
            "'Labels vs Counts - test.csv'\n",
            "'Labels vs Counts - train.csv'\n",
            "'Labels vs Counts - validation.csv'\n",
            "'Number of labels vs number of sentences - test.csv'\n",
            "'Number of labels vs number of sentences - train.csv'\n",
            "'Number of labels vs number of sentences - validation.csv'\n",
            " sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB9MR3KcWXUs",
        "outputId": "81ecb5bb-4cad-4570-886b-f5f9a6791f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nlpaueb/sec-bert-num were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/sec-bert-num and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=170, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained('nlpaueb/sec-bert-num', num_labels=170)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6vtGhfHK-n-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "7efb6354-45a6-48f9-83d0-135ce6e5cda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=24bfcf5e20a76b728f628379c1e7c0747b9d249c99b9107189716d57a8516c85\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e0a9e0e9810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mGPUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGPUs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# XXX: only one GPU on Colab and isn’t guaranteed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPUs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsAA_jT-p2Bw",
        "outputId": "51323f8b-024b-4a6e-a7c9-0944a0dd67d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp7Yl4JyWhDj"
      },
      "source": [
        "#### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqAN7YVIjKTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4542e0da-c87a-46f2-f1b3-59652e76c0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.1274, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "print(initial_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-z6YCpGnvfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0f3913-a36d-4355-9d1c-3dc1e509f43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512, 170])\n"
          ]
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "print(tr_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kznSQfGIWdU4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIVVfFHi7Aw7"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in tqdm(enumerate(testing_loader)):\n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            loss, eval_logits = outputs[0], outputs[1]\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                # print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [feature_names[id.item()] for id in eval_labels]\n",
        "    predictions = [feature_names[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLFivpkwW1HY"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in tqdm(enumerate(training_loader)):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, attention_mask=mask, labels=labels)\n",
        "        loss, tr_logits = outputs[0], outputs[1]\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 500==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "        \n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_labels.extend(labels)\n",
        "        tr_preds.extend(predictions)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if idx%3000==0:\n",
        "          val_labels, val_predictions = valid(model, validating_loader)\n",
        "          print(classification_report([val_labels], [val_predictions]))\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IigJPZ8QHrCO"
      },
      "outputs": [],
      "source": [
        "for idx, batch in tqdm(enumerate(training_loader)):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic9wxh4uHzd1"
      },
      "outputs": [],
      "source": [
        "len(batch['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y07Ybw8rZeZ7"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)\n",
        "    val_labels, val_predictions = valid(model, validating_loader)\n",
        "    print(classification_report([val_labels], [val_predictions]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm-GA6lb8Vgk"
      },
      "source": [
        "#### **Test and Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLdJ21SR8agL"
      },
      "outputs": [],
      "source": [
        "total_count = dict()\n",
        "correct_count = dict()\n",
        "\n",
        "def test(model, testing_loader):\n",
        "    \n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    nb_test_examples, nb_test_steps = 0, 0\n",
        "    test_preds, test_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in tqdm(enumerate(testing_loader)):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            loss, test_logits = outputs[0], outputs[1]\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "\n",
        "            nb_test_steps += 1\n",
        "            nb_test_examples += labels.size(0)\n",
        "              \n",
        "            # compute test accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = test_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            count = np.count_nonzero(labels.cpu().numpy())\n",
        "            if count in total_count.keys():\n",
        "              total_count[count] += len(labels.cpu().numpy())\n",
        "            else:\n",
        "              total_count[count] = len(labels.cpu().numpy())\n",
        "            \n",
        "            if count in correct_count.keys():\n",
        "              correct_count[count] += np.sum(labels.cpu().numpy() == predictions.cpu().numpy())\n",
        "            else:\n",
        "              correct_count[count] = np.sum(labels.cpu().numpy() == predictions.cpu().numpy())\n",
        "            \n",
        "            test_labels.extend(labels)\n",
        "            test_preds.extend(predictions)\n",
        "            \n",
        "            tmp_test_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            test_accuracy += tmp_test_accuracy\n",
        "\n",
        "    labels = [feature_names[id.item()] for id in test_labels]\n",
        "    predictions = [feature_names[id.item()] for id in test_preds]\n",
        "    \n",
        "    test_loss = test_loss / nb_test_steps\n",
        "    test_accuracy = test_accuracy / nb_test_steps\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUh9PHlh9QU3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "unique_feature_names = np.unique(feature_names)\n",
        "\n",
        "labels, predictions = test(model, testing_loader)\n",
        "print(labels)\n",
        "conf_matrix = confusion_matrix(labels, predictions, labels = unique_feature_names)\n",
        "acc_col = (conf_matrix.diagonal()*100)/conf_matrix.sum(axis=1)\n",
        "\n",
        "test_cols = np.unique(labels, return_counts=True)\n",
        "test_df1 = pd.DataFrame(list(zip(test_cols[0], test_cols[1])), columns=['class', 'count_sample'])\n",
        "test_df2 = pd.DataFrame(list(zip(unique_feature_names, acc_col)), columns=['class', 'accuracy'])\n",
        "final_result_df = test_df1.merge(test_df2, on='class', how='right')\n",
        "final_result_df.to_csv('Performance according to the number of data points in train per label.csv')\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MhHd2LktaSy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for key in sorted(total_count.keys()):\n",
        "  x.append(key)\n",
        "  y.append((100*correct_count[key])/total_count[key])\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('Number of labels in a sentence')\n",
        "plt.ylabel('accuracy')\n",
        " \n",
        "plt.title(' Performance as number of labels per sentence increases')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqDklprSqB5d"
      },
      "source": [
        "#### **Saving the model for future use**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDZtSsKKntuI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "directory = \"./sec-bert-num\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# save vocabulary of the tokenizer\n",
        "tokenizer.save_vocabulary(directory)\n",
        "# save the model weights and its configuration file\n",
        "model.save_pretrained(directory)\n",
        "print('All files saved')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "sec-bert-num v2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}